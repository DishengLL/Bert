# Bert

this repository record all of material/note during learning bert


## Bert stands in the shoulder of giants 
- ULM-Fit (January)
- ELMo (February)
- OpenAi GPT (June)
- Bert (October)

#### ELMo
from [ELMo](https://arxiv.org/abs/1802.05365) (Embeddings from Language Model, 2017) to Bert 
(Bidirectional Encoder Representation with Transformers, 2018)

ELMo understand word/vocabulary via bidirectional method (from left to right & from right to left), differing from 
traditional methods like RNNs, LSTMs, etc.
ElMo is stronger than the traditional unidirectional model, while it is less powerful than Transformer in the 
**long-term dependency**.

#### ULM-Fit
ULM-Fit stands for [*Universal Language Model Fine-tuning*](https://arxiv.org/pdf/1801.06146.pdf) 

#### GPT
GPT stands for *Generative Pre-trained Transformer* (a decoder component of transformer)

### reference
[web blog](https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial)
